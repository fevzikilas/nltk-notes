{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb747f0",
   "metadata": {},
   "source": [
    "# Natural Language Processing with NLTK\n",
    "\n",
    "This notebook explores fundamental Natural Language Processing (NLP) techniques using the Natural Language Toolkit (NLTK) library. We'll cover tokenization, stemming, lemmatization, stopword removal, part-of-speech tagging, and named entity recognition.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#intro)\n",
    "2. [Text Preprocessing](#preprocessing)\n",
    "   - [Tokenization](#tokenization)\n",
    "   - [Stemming](#stemming)\n",
    "   - [Lemmatization](#lemmatization)\n",
    "   - [Stopword Removal](#stopwords)\n",
    "3. [Advanced NLP Techniques](#advanced)\n",
    "   - [Part-of-Speech Tagging](#pos)\n",
    "   - [Named Entity Recognition](#ner)\n",
    "4. [Practical Applications](#applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d37c5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import NLTK and download the necessary resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d3dda-6020-428d-b8a4-7ece1d435c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107f8fc",
   "metadata": {},
   "source": [
    "## Sample Text Data\n",
    "\n",
    "Let's create a sample corpus to work with throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e3c91374-94cf-4678-a885-2a8d2f540591",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"This is a sample text corpus.\n",
    "It contains multiple sentences.\n",
    "The purpose of this corpus is to demonstrate text processing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63478a3b-c447-4675-bab8-3de8ecb3b9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample corpus:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is a sample text corpus.\\nIt contains multiple sentences.\\nThe purpose of this corpus is to demonstrate text processing.\\n'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the corpus\n",
    "print(\"Sample corpus:\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63765abb",
   "metadata": {},
   "source": [
    "## Tokenization <a id=\"tokenization\"></a>\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, such as sentences or words. This is typically the first step in any NLP pipeline.\n",
    "\n",
    "### Sentence Tokenization\n",
    "\n",
    "Sentence tokenization splits a paragraph or document into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d6c956-43ff-4c51-a060-3db9aa8b4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3\n",
      "Sentence 1: This is a sample text corpus.\n",
      "Sentence 2: It contains multiple sentences.\n",
      "Sentence 3: The purpose of this corpus is to demonstrate text processing.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Tokenize corpus into sentences\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef3a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of tokenized sentences: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Check type of document\n",
    "print(f\"Type of tokenized sentences: {type(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72629d0e",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "\n",
    "Word tokenization splits sentences into individual words. NLTK offers several tokenizers with different behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd36cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenization results:\n",
      "Sentence 1: ['This', 'is', 'a', 'sample', 'text', 'corpus', '.']\n",
      "Sentence 2: ['It', 'contains', 'multiple', 'sentences', '.']\n",
      "Sentence 3: ['The', 'purpose', 'of', 'this', 'corpus', 'is', 'to', 'demonstrate', 'text', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization (sentence â†’ words)\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "print(\"Word tokenization results:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    words = word_tokenize(sentence)\n",
    "    print(f\"Sentence {i}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db266398",
   "metadata": {},
   "source": [
    "### Comparing Different Tokenizers\n",
    "\n",
    "NLTK provides various tokenizers, each with different rules and behaviors. Let's compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20ae55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_tokenize</th>\n",
       "      <td>Do</td>\n",
       "      <td>n't</td>\n",
       "      <td>hesitate</td>\n",
       "      <td>to</td>\n",
       "      <td>email</td>\n",
       "      <td>me</td>\n",
       "      <td>at</td>\n",
       "      <td>john.doe</td>\n",
       "      <td>@</td>\n",
       "      <td>example.com</td>\n",
       "      <td>...</td>\n",
       "      <td>!</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wordpunct_tokenize</th>\n",
       "      <td>Don</td>\n",
       "      <td>'</td>\n",
       "      <td>t</td>\n",
       "      <td>hesitate</td>\n",
       "      <td>to</td>\n",
       "      <td>email</td>\n",
       "      <td>me</td>\n",
       "      <td>at</td>\n",
       "      <td>john</td>\n",
       "      <td>.</td>\n",
       "      <td>...</td>\n",
       "      <td>com</td>\n",
       "      <td>or</td>\n",
       "      <td>call</td>\n",
       "      <td>at</td>\n",
       "      <td>555</td>\n",
       "      <td>-</td>\n",
       "      <td>123</td>\n",
       "      <td>-</td>\n",
       "      <td>4567</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TreebankWordTokenizer</th>\n",
       "      <td>Do</td>\n",
       "      <td>n't</td>\n",
       "      <td>hesitate</td>\n",
       "      <td>to</td>\n",
       "      <td>email</td>\n",
       "      <td>me</td>\n",
       "      <td>at</td>\n",
       "      <td>john.doe</td>\n",
       "      <td>@</td>\n",
       "      <td>example.com</td>\n",
       "      <td>...</td>\n",
       "      <td>!</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0    1         2         3      4      5   6   \\\n",
       "word_tokenize           Do  n't  hesitate        to  email     me  at   \n",
       "wordpunct_tokenize     Don    '         t  hesitate     to  email  me   \n",
       "TreebankWordTokenizer   Do  n't  hesitate        to  email     me  at   \n",
       "\n",
       "                             7     8            9   ...   14    15    16  \\\n",
       "word_tokenize          john.doe     @  example.com  ...    !  None  None   \n",
       "wordpunct_tokenize           at  john            .  ...  com    or  call   \n",
       "TreebankWordTokenizer  john.doe     @  example.com  ...    !  None  None   \n",
       "\n",
       "                         17    18    19    20    21    22    23  \n",
       "word_tokenize          None  None  None  None  None  None  None  \n",
       "wordpunct_tokenize       at   555     -   123     -  4567     !  \n",
       "TreebankWordTokenizer  None  None  None  None  None  None  None  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, TreebankWordTokenizer\n",
    "import pandas as pd \n",
    "# Sample text for comparison\n",
    "sample = \"Don't hesitate to email me at john.doe@example.com or call at 555-123-4567!\"\n",
    "\n",
    "# Compare different tokenizers\n",
    "tokenizers = {\n",
    "    'word_tokenize': word_tokenize,\n",
    "    'wordpunct_tokenize': wordpunct_tokenize,\n",
    "    'TreebankWordTokenizer': TreebankWordTokenizer().tokenize\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "results = {}\n",
    "max_length = 0\n",
    "\n",
    "# Tokenize and find the maximum length of tokenized results\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokenized = tokenizer(sample)\n",
    "    results[name] = tokenized\n",
    "    max_length = max(max_length, len(tokenized))\n",
    "\n",
    "# Pad tokenized results to make all arrays the same length\n",
    "for name in results:\n",
    "    results[name] += [None] * (max_length - len(results[name]))\n",
    "\n",
    "# Display results as a DataFrame\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd71358",
   "metadata": {},
   "source": [
    "## Stemming <a id=\"stemming\"></a>\n",
    "\n",
    "Stemming is the process of reducing words to their word stem or root form. It's a rule-based process that chops off the ends of words to remove affixes. Stemming is useful for text normalization, but often produces non-dictionary words.\n",
    "\n",
    "### Comparing Different Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, RegexpStemmer, SnowballStemmer\n",
    "\n",
    "# Create sample words to compare stemmers\n",
    "words = [\"fearly\", \"running\", \"ran\", \"easily\", \"fairness\", \"eating\", \"eats\", \"eater\", \"eat\", \"history\", \"historical\", \"congratulations\", \"sliding\", \"comfortable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c9d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Porter</th>\n",
       "      <th>RegExp</th>\n",
       "      <th>Snowball</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fearly</td>\n",
       "      <td>fearli</td>\n",
       "      <td>fearly</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>runn</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>easily</td>\n",
       "      <td>easili</td>\n",
       "      <td>easily</td>\n",
       "      <td>easili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fairness</td>\n",
       "      <td>fair</td>\n",
       "      <td>fairnes</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eating</td>\n",
       "      <td>eat</td>\n",
       "      <td>eat</td>\n",
       "      <td>eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eats</td>\n",
       "      <td>eat</td>\n",
       "      <td>eat</td>\n",
       "      <td>eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>eater</td>\n",
       "      <td>eater</td>\n",
       "      <td>eater</td>\n",
       "      <td>eater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eat</td>\n",
       "      <td>eat</td>\n",
       "      <td>eat</td>\n",
       "      <td>eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>history</td>\n",
       "      <td>histori</td>\n",
       "      <td>history</td>\n",
       "      <td>histori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>historical</td>\n",
       "      <td>histor</td>\n",
       "      <td>historical</td>\n",
       "      <td>histor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>congratulations</td>\n",
       "      <td>congratul</td>\n",
       "      <td>congratulation</td>\n",
       "      <td>congratul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sliding</td>\n",
       "      <td>slide</td>\n",
       "      <td>slid</td>\n",
       "      <td>slide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>comfortable</td>\n",
       "      <td>comfort</td>\n",
       "      <td>comfort</td>\n",
       "      <td>comfort</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Original     Porter          RegExp   Snowball\n",
       "0            fearly     fearli          fearly       fear\n",
       "1           running        run            runn        run\n",
       "2               ran        ran             ran        ran\n",
       "3            easily     easili          easily     easili\n",
       "4          fairness       fair         fairnes       fair\n",
       "5            eating        eat             eat        eat\n",
       "6              eats        eat             eat        eat\n",
       "7             eater      eater           eater      eater\n",
       "8               eat        eat             eat        eat\n",
       "9           history    histori         history    histori\n",
       "10       historical     histor      historical     histor\n",
       "11  congratulations  congratul  congratulation  congratul\n",
       "12          sliding      slide            slid      slide\n",
       "13      comfortable    comfort         comfort    comfort"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Initialize stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "regexp_stemmer = RegexpStemmer('ing$|s$|e$|able', min=4)\n",
    "snowball_stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "# Create comparison table\n",
    "stemming_results = {\n",
    "    'Original': words,\n",
    "    'Porter': [porter_stemmer.stem(word) for word in words],\n",
    "    'RegExp': [regexp_stemmer.stem(word) for word in words],\n",
    "    'Snowball': [snowball_stemmer.stem(word) for word in words]\n",
    "}\n",
    "\n",
    "# Display results\n",
    "stemming_df = pd.DataFrame(stemming_results)\n",
    "stemming_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11097cf7",
   "metadata": {},
   "source": [
    "### Stemming Analysis\n",
    "\n",
    "As you can see from the results:\n",
    "\n",
    "1. **Porter Stemmer**: One of the oldest and simplest stemmers, it applies a set of rules to remove suffixes.\n",
    "2. **RegExp Stemmer**: Uses regular expressions to strip specified patterns from the end of words. It's simple but less comprehensive.\n",
    "3. **Snowball Stemmer**: An improved version of the Porter algorithm, also known as Porter2, offering better accuracy for English and support for multiple languages.\n",
    "\n",
    "Notice how stemming can sometimes produce non-dictionary words (e.g., \"histori\" for \"history\"). This is one of the main drawbacks of stemming compared to lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a576a2a",
   "metadata": {},
   "source": [
    "## Lemmatization <a id=\"lemmatization\"></a>\n",
    "\n",
    "Lemmatization is similar to stemming, but it reduces words to their dictionary form (lemma) rather than just chopping off affixes. It considers the morphological analysis of the words and produces actual dictionary words.\n",
    "\n",
    "Lemmatization is often preferred for applications like chatbots, Q&A systems, and text summarization because it preserves the meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "b5565da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5875dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Lemma (noun)</th>\n",
       "      <th>Lemma (verb)</th>\n",
       "      <th>Lemma (adjective)</th>\n",
       "      <th>Lemma (adverb)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>running</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "      <td>run</td>\n",
       "      <td>ran</td>\n",
       "      <td>ran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>good</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>studies</td>\n",
       "      <td>study</td>\n",
       "      <td>study</td>\n",
       "      <td>studies</td>\n",
       "      <td>studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>studied</td>\n",
       "      <td>studied</td>\n",
       "      <td>study</td>\n",
       "      <td>studied</td>\n",
       "      <td>studied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feet</td>\n",
       "      <td>foot</td>\n",
       "      <td>feet</td>\n",
       "      <td>feet</td>\n",
       "      <td>feet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>children</td>\n",
       "      <td>child</td>\n",
       "      <td>children</td>\n",
       "      <td>children</td>\n",
       "      <td>children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>geese</td>\n",
       "      <td>goose</td>\n",
       "      <td>geese</td>\n",
       "      <td>geese</td>\n",
       "      <td>geese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mice</td>\n",
       "      <td>mouse</td>\n",
       "      <td>mice</td>\n",
       "      <td>mice</td>\n",
       "      <td>mice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>was</td>\n",
       "      <td>wa</td>\n",
       "      <td>be</td>\n",
       "      <td>was</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fairly</td>\n",
       "      <td>fairly</td>\n",
       "      <td>fairly</td>\n",
       "      <td>fairly</td>\n",
       "      <td>fairly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Original Lemma (noun) Lemma (verb) Lemma (adjective) Lemma (adverb)\n",
       "0    running      running          run           running        running\n",
       "1        ran          ran          run               ran            ran\n",
       "2     better       better       better              good           well\n",
       "3    studies        study        study           studies        studies\n",
       "4    studied      studied        study           studied        studied\n",
       "5       feet         foot         feet              feet           feet\n",
       "6   children        child     children          children       children\n",
       "7      geese        goose        geese             geese          geese\n",
       "8       mice        mouse         mice              mice           mice\n",
       "9        are          are           be               are            are\n",
       "10        is           is           be                is             is\n",
       "11       was           wa           be               was            was\n",
       "12    fairly       fairly       fairly            fairly         fairly"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Lemmatization with different POS (Part-of-Speech) tags\n",
    "# POS tags: n-noun, v-verb, a-adjective, r-adverb\n",
    "pos_tags = {'n': 'noun', 'v': 'verb', 'a': 'adjective', 'r': 'adverb'}\n",
    "\n",
    "# Test words for lemmatization\n",
    "lemma_words = [\"running\", \"ran\", \"better\", \"studies\", \"studied\", \"feet\", \"children\", \"geese\", \"mice\", \"are\", \"is\", \"was\", \"fairly\"]\n",
    "\n",
    "# Create comparison table for different POS tags\n",
    "lemma_results = {'Original': lemma_words}\n",
    "\n",
    "for pos_tag, name in pos_tags.items():\n",
    "    lemma_results[f'Lemma ({name})'] = [lemmatizer.lemmatize(word, pos=pos_tag) for word in lemma_words]\n",
    "\n",
    "# Display results\n",
    "pd.DataFrame(lemma_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7de21",
   "metadata": {},
   "source": [
    "### Stemming vs. Lemmatization\n",
    "\n",
    "Let's compare stemming and lemmatization side by side to see the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2d4c0492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Porter Stemmer</th>\n",
       "      <th>Snowball Stemmer</th>\n",
       "      <th>Lemmatization (verb)</th>\n",
       "      <th>Lemmatization (noun)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>run</td>\n",
       "      <td>run</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>studies</td>\n",
       "      <td>studi</td>\n",
       "      <td>studi</td>\n",
       "      <td>study</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feet</td>\n",
       "      <td>feet</td>\n",
       "      <td>feet</td>\n",
       "      <td>feet</td>\n",
       "      <td>foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wolves</td>\n",
       "      <td>wolv</td>\n",
       "      <td>wolv</td>\n",
       "      <td>wolves</td>\n",
       "      <td>wolf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>historically</td>\n",
       "      <td>histor</td>\n",
       "      <td>histor</td>\n",
       "      <td>historically</td>\n",
       "      <td>historically</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Original Porter Stemmer Snowball Stemmer Lemmatization (verb)  \\\n",
       "0       running            run              run                  run   \n",
       "1        better         better           better               better   \n",
       "2       studies          studi            studi                study   \n",
       "3          feet           feet             feet                 feet   \n",
       "4        wolves           wolv             wolv               wolves   \n",
       "5           are            are              are                   be   \n",
       "6  historically         histor           histor         historically   \n",
       "\n",
       "  Lemmatization (noun)  \n",
       "0              running  \n",
       "1               better  \n",
       "2                study  \n",
       "3                 foot  \n",
       "4                 wolf  \n",
       "5                  are  \n",
       "6         historically  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare stemming vs lemmatization\n",
    "compare_words = [\"running\", \"better\", \"studies\", \"feet\", \"wolves\", \"are\", \"historically\"]\n",
    "\n",
    "comparison = {\n",
    "    'Original': compare_words,\n",
    "    'Porter Stemmer': [porter_stemmer.stem(word) for word in compare_words],\n",
    "    'Snowball Stemmer': [snowball_stemmer.stem(word) for word in compare_words],\n",
    "    'Lemmatization (verb)': [lemmatizer.lemmatize(word, pos='v') for word in compare_words],\n",
    "    'Lemmatization (noun)': [lemmatizer.lemmatize(word, pos='n') for word in compare_words]\n",
    "}\n",
    "\n",
    "pd.DataFrame(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0becf8e",
   "metadata": {},
   "source": [
    "## Stopword Removal <a id=\"stopwords\"></a>\n",
    "\n",
    "Stopwords are common words like \"the\", \"a\", \"an\", \"in\" that usually don't carry much meaning in text analysis. Removing them can help reduce noise in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c8466c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample paragraph for stopword removal\n",
    "paragraph = \"\"\"On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone beforeâ€”the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. The moonwalk took place the following day. On July 21, 1969, at precisely 10:56 EDT, Commander Neil Armstrong emerged from the Lunar Module and took his famous first step onto the moon's surface. He declared, \n",
    " It was a monumental moment in human history!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e2e66375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English stopwords: 198\n",
      "Sample stopwords: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Display first 20 stopwords\n",
    "print(f\"Total English stopwords: {len(stop_words)}\")\n",
    "print(f\"Sample stopwords: {stop_words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7235c468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original with Stopwords</th>\n",
       "      <th>After Stopwords Removal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On July 16 , 1969 , the Apollo 11 spacecraft l...</td>\n",
       "      <td>July 16 , 1969 , Apollo 11 spacecraft launched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Its mission was to go where no human being had...</td>\n",
       "      <td>mission go human gone beforeâ€”the moon !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The crew consisted of Neil Armstrong , Michael...</td>\n",
       "      <td>crew consisted Neil Armstrong , Michael Collin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Original with Stopwords  \\\n",
       "0  On July 16 , 1969 , the Apollo 11 spacecraft l...   \n",
       "1  Its mission was to go where no human being had...   \n",
       "2  The crew consisted of Neil Armstrong , Michael...   \n",
       "\n",
       "                             After Stopwords Removal  \n",
       "0  July 16 , 1969 , Apollo 11 spacecraft launched...  \n",
       "1            mission go human gone beforeâ€”the moon !  \n",
       "2  crew consisted Neil Armstrong , Michael Collin...  "
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process text with and without stopwords removal\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Initialize lists for processed sentences\n",
    "with_stopwords = []\n",
    "without_stopwords = []\n",
    "\n",
    "# Process each sentence\n",
    "for sentence in sentences[:3]:  # Process first 3 sentences for brevity\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Keep all words\n",
    "    with_stopwords.append(' '.join(words))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    without_stopwords.append(' '.join(filtered_words))\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "stopword_df = pd.DataFrame({\n",
    "    'Original with Stopwords': with_stopwords,\n",
    "    'After Stopwords Removal': without_stopwords\n",
    "})\n",
    "\n",
    "stopword_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8a931",
   "metadata": {},
   "source": [
    "### Complete Text Processing Pipeline\n",
    "\n",
    "Let's put everything together to create a complete text processing pipeline that includes tokenization, stopword removal, and either stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "  Stemmed: juli 16 1969 apollo 11 spacecraft launch kennedi space center florida\n",
      "  Lemmatized: july 16 1969 apollo 11 spacecraft launch kennedy space center florida\n",
      "\n",
      "Sentence 2:\n",
      "  Stemmed: mission go human gone moon\n",
      "  Lemmatized: mission go human go moon\n",
      "\n",
      "Sentence 3:\n",
      "  Stemmed: crew consist neil armstrong michael collin buzz aldrin\n",
      "  Lemmatized: crew consist neil armstrong michael collins buzz aldrin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_text(text, use_stemming=True, use_lemmatization=False):\n",
    "    \"\"\"Process text using a complete NLP pipeline\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to process\n",
    "        use_stemming (bool): Whether to apply stemming\n",
    "        use_lemmatization (bool): Whether to apply lemmatization\n",
    "        \n",
    "    Returns:\n",
    "        list: List of processed sentences\n",
    "    \"\"\"\n",
    "    # Tokenize into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        filtered_words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word.isalnum()]\n",
    "        \n",
    "        # Apply stemming or lemmatization\n",
    "        if use_stemming:\n",
    "            processed_words = [snowball_stemmer.stem(word) for word in filtered_words]\n",
    "        elif use_lemmatization:\n",
    "            processed_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n",
    "        else:\n",
    "            processed_words = filtered_words\n",
    "            \n",
    "        processed_sentences.append(' '.join(processed_words))\n",
    "        \n",
    "    return processed_sentences\n",
    "\n",
    "# Process the paragraph\n",
    "stemmed_text = process_text(paragraph, use_stemming=True, use_lemmatization=False)\n",
    "lemmatized_text = process_text(paragraph, use_stemming=False, use_lemmatization=True)\n",
    "\n",
    "# Display first 3 processed sentences\n",
    "for i, (stem, lemma) in enumerate(zip(stemmed_text[:3], lemmatized_text[:3])):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    print(f\"  Stemmed: {stem}\")\n",
    "    print(f\"  Lemmatized: {lemma}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075fea97",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging <a id=\"pos\"></a>\n",
    "\n",
    "POS tagging is the process of marking words in a text with their corresponding part of speech (noun, verb, adjective, etc.). It's an essential step for many NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "78dffca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC</td>\n",
       "      <td>Coordinating conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CD</td>\n",
       "      <td>Cardinal digit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DT</td>\n",
       "      <td>Determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EX</td>\n",
       "      <td>Existential there (\"there is\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FW</td>\n",
       "      <td>Foreign word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IN</td>\n",
       "      <td>Preposition/subordinating conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JJR</td>\n",
       "      <td>Adjective, comparative (\"bigger\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JJS</td>\n",
       "      <td>Adjective, superlative (\"biggest\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LS</td>\n",
       "      <td>List marker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MD</td>\n",
       "      <td>Modal (could, will)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NN</td>\n",
       "      <td>Noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NNS</td>\n",
       "      <td>Noun plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NNP</td>\n",
       "      <td>Proper noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NNPS</td>\n",
       "      <td>Proper noun, plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PDT</td>\n",
       "      <td>Predeterminer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>POS</td>\n",
       "      <td>Possessive ending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PRP</td>\n",
       "      <td>Personal pronoun (I, he, she)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PRP$</td>\n",
       "      <td>Possessive pronoun (my, his, hers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RB</td>\n",
       "      <td>Adverb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RBR</td>\n",
       "      <td>Adverb, comparative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RBS</td>\n",
       "      <td>Adverb, superlative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RP</td>\n",
       "      <td>Particle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>UH</td>\n",
       "      <td>Interjection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>VB</td>\n",
       "      <td>Verb, base form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>VBD</td>\n",
       "      <td>Verb, past tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>VBG</td>\n",
       "      <td>Verb, gerund/present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VBN</td>\n",
       "      <td>Verb, past participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>VBP</td>\n",
       "      <td>Verb, sing. present, non-3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>VBZ</td>\n",
       "      <td>Verb, 3rd person sing. present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>WDT</td>\n",
       "      <td>Wh-determiner (which)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>WP</td>\n",
       "      <td>Wh-pronoun (who, what)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>WP$</td>\n",
       "      <td>Possessive wh-pronoun (whose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WRB</td>\n",
       "      <td>Wh-adverb (where, when)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tag                            Description\n",
       "0     CC               Coordinating conjunction\n",
       "1     CD                         Cardinal digit\n",
       "2     DT                             Determiner\n",
       "3     EX         Existential there (\"there is\")\n",
       "4     FW                           Foreign word\n",
       "5     IN  Preposition/subordinating conjunction\n",
       "6     JJ                              Adjective\n",
       "7    JJR      Adjective, comparative (\"bigger\")\n",
       "8    JJS     Adjective, superlative (\"biggest\")\n",
       "9     LS                            List marker\n",
       "10    MD                    Modal (could, will)\n",
       "11    NN                         Noun, singular\n",
       "12   NNS                            Noun plural\n",
       "13   NNP                  Proper noun, singular\n",
       "14  NNPS                    Proper noun, plural\n",
       "15   PDT                          Predeterminer\n",
       "16   POS                      Possessive ending\n",
       "17   PRP          Personal pronoun (I, he, she)\n",
       "18  PRP$     Possessive pronoun (my, his, hers)\n",
       "19    RB                                 Adverb\n",
       "20   RBR                    Adverb, comparative\n",
       "21   RBS                    Adverb, superlative\n",
       "22    RP                               Particle\n",
       "23    TO                                     to\n",
       "24    UH                           Interjection\n",
       "25    VB                        Verb, base form\n",
       "26   VBD                       Verb, past tense\n",
       "27   VBG        Verb, gerund/present participle\n",
       "28   VBN                  Verb, past participle\n",
       "29   VBP            Verb, sing. present, non-3d\n",
       "30   VBZ         Verb, 3rd person sing. present\n",
       "31   WDT                  Wh-determiner (which)\n",
       "32    WP                 Wh-pronoun (who, what)\n",
       "33   WP$          Possessive wh-pronoun (whose)\n",
       "34   WRB                Wh-adverb (where, when)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common POS tags in NLTK\n",
    "pos_tags_info = {\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    'CD': 'Cardinal digit',\n",
    "    'DT': 'Determiner',\n",
    "    'EX': 'Existential there (\"there is\")',\n",
    "    'FW': 'Foreign word',\n",
    "    'IN': 'Preposition/subordinating conjunction',\n",
    "    'JJ': 'Adjective',\n",
    "    'JJR': 'Adjective, comparative (\"bigger\")',\n",
    "    'JJS': 'Adjective, superlative (\"biggest\")',\n",
    "    'LS': 'List marker',\n",
    "    'MD': 'Modal (could, will)',\n",
    "    'NN': 'Noun, singular',\n",
    "    'NNS': 'Noun plural',\n",
    "    'NNP': 'Proper noun, singular',\n",
    "    'NNPS': 'Proper noun, plural',\n",
    "    'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending',\n",
    "    'PRP': 'Personal pronoun (I, he, she)',\n",
    "    'PRP$': 'Possessive pronoun (my, his, hers)',\n",
    "    'RB': 'Adverb',\n",
    "    'RBR': 'Adverb, comparative',\n",
    "    'RBS': 'Adverb, superlative',\n",
    "    'RP': 'Particle',\n",
    "    'TO': 'to',\n",
    "    'UH': 'Interjection',\n",
    "    'VB': 'Verb, base form',\n",
    "    'VBD': 'Verb, past tense',\n",
    "    'VBG': 'Verb, gerund/present participle',\n",
    "    'VBN': 'Verb, past participle',\n",
    "    'VBP': 'Verb, sing. present, non-3d',\n",
    "    'VBZ': 'Verb, 3rd person sing. present',\n",
    "    'WDT': 'Wh-determiner (which)',\n",
    "    'WP': 'Wh-pronoun (who, what)',\n",
    "    'WP$': 'Possessive wh-pronoun (whose)',\n",
    "    'WRB': 'Wh-adverb (where, when)'\n",
    "}\n",
    "\n",
    "# Display POS tag information as a table\n",
    "pos_df = pd.DataFrame([(tag, desc) for tag, desc in pos_tags_info.items()], \n",
    "                      columns=['Tag', 'Description'])\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "76b2f6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: The quick brown fox jumps over the lazy dog.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>Determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quick</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brown</td>\n",
       "      <td>NN</td>\n",
       "      <td>Noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fox</td>\n",
       "      <td>NN</td>\n",
       "      <td>Noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jumps</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>Verb, 3rd person sing. present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>over</td>\n",
       "      <td>IN</td>\n",
       "      <td>Preposition/subordinating conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>Determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lazy</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog</td>\n",
       "      <td>NN</td>\n",
       "      <td>Noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word POS Tag                            Description\n",
       "0    The      DT                             Determiner\n",
       "1  quick      JJ                              Adjective\n",
       "2  brown      NN                         Noun, singular\n",
       "3    fox      NN                         Noun, singular\n",
       "4  jumps     VBZ         Verb, 3rd person sing. present\n",
       "5   over      IN  Preposition/subordinating conjunction\n",
       "6    the      DT                             Determiner\n",
       "7   lazy      JJ                              Adjective\n",
       "8    dog      NN                         Noun, singular\n",
       "9      .       .                                Unknown"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentence 2: I am studying natural language processing.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Personal pronoun (I, he, she)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am</td>\n",
       "      <td>VBP</td>\n",
       "      <td>Verb, sing. present, non-3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>studying</td>\n",
       "      <td>VBG</td>\n",
       "      <td>Verb, gerund/present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>natural</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>language</td>\n",
       "      <td>NN</td>\n",
       "      <td>Noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>processing</td>\n",
       "      <td>NN</td>\n",
       "      <td>Noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS Tag                      Description\n",
       "0           I     PRP    Personal pronoun (I, he, she)\n",
       "1          am     VBP      Verb, sing. present, non-3d\n",
       "2    studying     VBG  Verb, gerund/present participle\n",
       "3     natural      JJ                        Adjective\n",
       "4    language      NN                   Noun, singular\n",
       "5  processing      NN                   Noun, singular\n",
       "6           .       .                          Unknown"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentence 3: She walked to the store, but it was closed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Personal pronoun (I, he, she)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>walked</td>\n",
       "      <td>VBD</td>\n",
       "      <td>Verb, past tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>Determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store</td>\n",
       "      <td>NN</td>\n",
       "      <td>Noun, singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>but</td>\n",
       "      <td>CC</td>\n",
       "      <td>Coordinating conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it</td>\n",
       "      <td>PRP</td>\n",
       "      <td>Personal pronoun (I, he, she)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>was</td>\n",
       "      <td>VBD</td>\n",
       "      <td>Verb, past tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>closed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>Verb, past participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word POS Tag                    Description\n",
       "0      She     PRP  Personal pronoun (I, he, she)\n",
       "1   walked     VBD               Verb, past tense\n",
       "2       to      TO                             to\n",
       "3      the      DT                     Determiner\n",
       "4    store      NN                 Noun, singular\n",
       "5        ,       ,                        Unknown\n",
       "6      but      CC       Coordinating conjunction\n",
       "7       it     PRP  Personal pronoun (I, he, she)\n",
       "8      was     VBD               Verb, past tense\n",
       "9   closed     VBN          Verb, past participle\n",
       "10       .       .                        Unknown"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example sentences for POS tagging\n",
    "example_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I am studying natural language processing.\",\n",
    "    \"She walked to the store, but it was closed.\"\n",
    "]\n",
    "\n",
    "# Perform POS tagging\n",
    "for i, sentence in enumerate(example_sentences, 1):\n",
    "    # Tokenize and tag words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    \n",
    "    # Create a visualization of the tagged sentence\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "    \n",
    "    # Display tagged words in a table format\n",
    "    tagged_df = pd.DataFrame(tagged, columns=['Word', 'POS Tag'])\n",
    "    tagged_df['Description'] = tagged_df['POS Tag'].map(lambda tag: pos_tags_info.get(tag, 'Unknown'))\n",
    "    display(tagged_df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb7244",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) <a id=\"ner\"></a>\n",
    "\n",
    "Named Entity Recognition is the process of identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, time expressions, quantities, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5fbe5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences for NER\n",
    "ner_examples = [\n",
    "    \"The Eiffel Tower stands on four lattice-girder piers that taper inward and join to form a single large vertical tower.\",\n",
    "    \"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.\",\n",
    "    \"Barack Obama was born in Hawaii and served as the 44th president of the United States from 2009 to 2017.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "ca0c465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: The Eiffel Tower stands on four lattice-girder piers that taper inward and join to form a single large vertical tower.\n",
      "\n",
      "Named Entities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eiffel Tower</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Entity          Type\n",
       "0  Eiffel Tower  ORGANIZATION"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example 2: Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.\n",
      "\n",
      "Named Entities:\n",
      "Example 2: Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.\n",
      "\n",
      "Named Entities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inc.</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steve Jobs</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steve Wozniak</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ronald Wayne</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entity          Type\n",
       "0          Apple        PERSON\n",
       "1           Inc.  ORGANIZATION\n",
       "2     Steve Jobs        PERSON\n",
       "3  Steve Wozniak        PERSON\n",
       "4   Ronald Wayne        PERSON"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example 3: Barack Obama was born in Hawaii and served as the 44th president of the United States from 2009 to 2017.\n",
      "\n",
      "Named Entities:\n",
      "Example 3: Barack Obama was born in Hawaii and served as the 44th president of the United States from 2009 to 2017.\n",
      "\n",
      "Named Entities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barack</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Obama</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entity    Type\n",
       "0         Barack  PERSON\n",
       "1          Obama  PERSON\n",
       "2         Hawaii     GPE\n",
       "3  United States     GPE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process examples with NER\n",
    "for i, example in enumerate(ner_examples, 1):\n",
    "    # Tokenize and tag\n",
    "    words = nltk.word_tokenize(example)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    \n",
    "    # Apply NER\n",
    "    ner_tree = nltk.ne_chunk(pos_tags)\n",
    "    \n",
    "    print(f\"Example {i}: {example}\")\n",
    "    print(\"\\nNamed Entities:\")\n",
    "    \n",
    "    # Extract and print named entities\n",
    "    named_entities = []\n",
    "    for chunk in ner_tree:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entity_name = ' '.join(c[0] for c in chunk)\n",
    "            entity_type = chunk.label()\n",
    "            named_entities.append((entity_name, entity_type))\n",
    "    \n",
    "    if named_entities:\n",
    "        entities_df = pd.DataFrame(named_entities, columns=['Entity', 'Type'])\n",
    "        display(entities_df)\n",
    "    else:\n",
    "        print(\"No named entities found\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "38b8a92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Tree for: Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.\n"
     ]
    }
   ],
   "source": [
    "# Visualize NER Tree (if svgling is installed)\n",
    "try:\n",
    "    import svgling\n",
    "    \n",
    "    # Use the second example for visualization\n",
    "    example = ner_examples[1]\n",
    "    words = nltk.word_tokenize(example)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    ner_tree = nltk.ne_chunk(pos_tags)\n",
    "    \n",
    "    print(f\"Named Entity Tree for: {example}\")\n",
    "    svgling.draw_tree(ner_tree)\n",
    "except ImportError:\n",
    "    print(\"To visualize NER trees, install the 'svgling' package using: pip install svgling\")\n",
    "    # Alternative visualization\n",
    "    print(ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec31d00",
   "metadata": {},
   "source": [
    "## Text Analysis Dashboard <a id=\"applications\"></a>\n",
    "\n",
    "Let's create a comprehensive analysis of text using various NLP techniques we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2139b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEXT ANALYSIS DASHBOARD ===\n",
      "Text length: 593 characters\n",
      "Sentences: 7\n",
      "Words: 100\n",
      "Unique words: 70\n",
      "Words without stopwords: 63\n",
      "\n",
      "=== MOST COMMON WORDS ===\n",
      "july: 3\n",
      "1969: 3\n",
      "moon: 3\n",
      "spacecraft: 2\n",
      "human: 2\n",
      "neil: 2\n",
      "armstrong: 2\n",
      "took: 2\n",
      "16: 1\n",
      "apollo: 1\n",
      "\n",
      "=== PART OF SPEECH DISTRIBUTION ===\n",
      "NN (Noun, singular): 32\n",
      "IN (Preposition/subordinating conjunction): 14\n",
      "DT (Determiner): 13\n",
      "VBD (Verb, past tense): 9\n",
      "JJ (Adjective): 8\n",
      "\n",
      "=== NAMED ENTITIES ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kennedy Space Center</td>\n",
       "      <td>FACILITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Florida</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neil Armstrong</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Collins</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buzz Aldrin</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sea</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tranquility</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Commander Neil Armstrong</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lunar Module</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Entity          Type\n",
       "0      Kennedy Space Center      FACILITY\n",
       "1                   Florida           GPE\n",
       "2            Neil Armstrong        PERSON\n",
       "3           Michael Collins        PERSON\n",
       "4               Buzz Aldrin        PERSON\n",
       "5                       Sea  ORGANIZATION\n",
       "6               Tranquility           GPE\n",
       "7  Commander Neil Armstrong  ORGANIZATION\n",
       "8              Lunar Module  ORGANIZATION"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Install wordcloud package for word cloud visualization: pip install wordcloud\n"
     ]
    }
   ],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Comprehensive text analysis using NLTK\"\"\"\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    \n",
    "    # Basic statistics\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words_lower = [word.lower() for word in words if word.isalnum()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_no_stop = [word for word in words_lower if word not in stop_words]\n",
    "    \n",
    "    # Word frequency\n",
    "    word_freq = Counter(words_no_stop)\n",
    "    common_words = word_freq.most_common(10)\n",
    "    \n",
    "    # POS distribution\n",
    "    pos_tags = nltk.pos_tag(words_lower)\n",
    "    pos_counts = Counter([tag for _, tag in pos_tags])\n",
    "    \n",
    "    # Named entities\n",
    "    ner_tree = nltk.ne_chunk(nltk.pos_tag(words))\n",
    "    named_entities = []\n",
    "    for chunk in ner_tree:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entity_name = ' '.join(c[0] for c in chunk)\n",
    "            entity_type = chunk.label()\n",
    "            named_entities.append((entity_name, entity_type))\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=== TEXT ANALYSIS DASHBOARD ===\")\n",
    "    print(f\"Text length: {len(text)} characters\")\n",
    "    print(f\"Sentences: {len(sentences)}\")\n",
    "    print(f\"Words: {len(words_lower)}\")\n",
    "    print(f\"Unique words: {len(set(words_lower))}\")\n",
    "    print(f\"Words without stopwords: {len(words_no_stop)}\")\n",
    "    \n",
    "    print(\"\\n=== MOST COMMON WORDS ===\")\n",
    "    for word, count in common_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    print(\"\\n=== PART OF SPEECH DISTRIBUTION ===\")\n",
    "    for pos, count in pos_counts.most_common(5):\n",
    "        print(f\"{pos} ({pos_tags_info.get(pos, 'Unknown')}): {count}\")\n",
    "    \n",
    "    print(\"\\n=== NAMED ENTITIES ===\")\n",
    "    if named_entities:\n",
    "        entities_df = pd.DataFrame(named_entities, columns=['Entity', 'Type'])\n",
    "        display(entities_df)\n",
    "    else:\n",
    "        print(\"No named entities found\")\n",
    "    \n",
    "    # Generate word cloud if matplotlib is available\n",
    "    try:\n",
    "        from wordcloud import WordCloud\n",
    "        \n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words_no_stop))\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud')\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"\\nInstall wordcloud package for word cloud visualization: pip install wordcloud\")\n",
    "\n",
    "# Run analysis on the Apollo 11 paragraph\n",
    "analyze_text(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751db7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c4608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388616bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
