[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Natural Language Processing with NLTK",
    "section": "",
    "text": "First, let’s install and import NLTK and download the necessary resources.\n\n! pip install nltk\n\nRequirement already satisfied: nltk in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (3.9.1)\nRequirement already satisfied: click in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (4.67.1)\nRequirement already satisfied: colorama in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from click-&gt;nltk) (0.4.6)\n\n\n\n[notice] A new release of pip is available: 24.2 -&gt; 25.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\n\nimport nltk\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping tokenizers\\punkt.zip.\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping tokenizers\\punkt.zip.\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Natural Language Processing with NLTK",
    "section": "",
    "text": "First, let’s install and import NLTK and download the necessary resources.\n\n! pip install nltk\n\nRequirement already satisfied: nltk in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (3.9.1)\nRequirement already satisfied: click in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex&gt;=2021.8.3 in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from nltk) (4.67.1)\nRequirement already satisfied: colorama in c:\\users\\fevzikilas\\desktop\\nlp\\nlp-l\\lib\\site-packages (from click-&gt;nltk) (0.4.6)\n\n\n\n[notice] A new release of pip is available: 24.2 -&gt; 25.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\n\nimport nltk\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping tokenizers\\punkt.zip.\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping tokenizers\\punkt.zip.\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to\n[nltk_data]     C:\\Users\\fevzikilas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package words is already up-to-date!\n\n\nTrue",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#sample-text-data",
    "href": "index.html#sample-text-data",
    "title": "Natural Language Processing with NLTK",
    "section": "Sample Text Data",
    "text": "Sample Text Data\nLet’s create a sample corpus to work with throughout this notebook.\n\ncorpus = \"\"\"This is a sample text corpus.\nIt contains multiple sentences.\nThe purpose of this corpus is to demonstrate text processing.\n\"\"\"\n\n\n# Display the corpus\nprint(\"Sample corpus:\")\ncorpus\n\nSample corpus:\n\n\n'This is a sample text corpus.\\nIt contains multiple sentences.\\nThe purpose of this corpus is to demonstrate text processing.\\n'",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#tokenization",
    "href": "index.html#tokenization",
    "title": "Natural Language Processing with NLTK",
    "section": "Tokenization ",
    "text": "Tokenization \nTokenization is the process of breaking down text into smaller units, such as sentences or words. This is typically the first step in any NLP pipeline.\n\nSentence Tokenization\nSentence tokenization splits a paragraph or document into individual sentences.\n\nfrom nltk.tokenize import sent_tokenize\n\n# Tokenize corpus into sentences\nsentences = sent_tokenize(corpus)\n\nprint(f\"Number of sentences: {len(sentences)}\")\nfor i, sentence in enumerate(sentences, 1):\n    print(f\"Sentence {i}: {sentence}\")\n\nNumber of sentences: 3\nSentence 1: This is a sample text corpus.\nSentence 2: It contains multiple sentences.\nSentence 3: The purpose of this corpus is to demonstrate text processing.\n\n\n\n# Check type of document\nprint(f\"Type of tokenized sentences: {type(sentences)}\")\n\nType of tokenized sentences: &lt;class 'list'&gt;\n\n\n\n\nWord Tokenization\nWord tokenization splits sentences into individual words. NLTK offers several tokenizers with different behaviors.\n\n# Word tokenization (sentence → words)\nfrom nltk.tokenize import word_tokenize\n\n# Tokenize each sentence into words\nprint(\"Word tokenization results:\")\nfor i, sentence in enumerate(sentences, 1):\n    words = word_tokenize(sentence)\n    print(f\"Sentence {i}: {words}\")\n\nWord tokenization results:\nSentence 1: ['This', 'is', 'a', 'sample', 'text', 'corpus', '.']\nSentence 2: ['It', 'contains', 'multiple', 'sentences', '.']\nSentence 3: ['The', 'purpose', 'of', 'this', 'corpus', 'is', 'to', 'demonstrate', 'text', 'processing', '.']\n\n\n\n\nComparing Different Tokenizers\nNLTK provides various tokenizers, each with different rules and behaviors. Let’s compare them:\n\nfrom nltk.tokenize import wordpunct_tokenize, TreebankWordTokenizer\nimport pandas as pd \n# Sample text for comparison\nsample = \"Don't hesitate to email me at john.doe@example.com or call at 555-123-4567!\"\n\n# Compare different tokenizers\ntokenizers = {\n    'word_tokenize': word_tokenize,\n    'wordpunct_tokenize': wordpunct_tokenize,\n    'TreebankWordTokenizer': TreebankWordTokenizer().tokenize\n}\n\n# Create a DataFrame to display results\nresults = {}\nmax_length = 0\n\n# Tokenize and find the maximum length of tokenized results\nfor name, tokenizer in tokenizers.items():\n    tokenized = tokenizer(sample)\n    results[name] = tokenized\n    max_length = max(max_length, len(tokenized))\n\n# Pad tokenized results to make all arrays the same length\nfor name in results:\n    results[name] += [None] * (max_length - len(results[name]))\n\n# Display results as a DataFrame\npd.DataFrame(results).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n\n\n\n\nword_tokenize\nDo\nn't\nhesitate\nto\nemail\nme\nat\njohn.doe\n@\nexample.com\n...\n!\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\nwordpunct_tokenize\nDon\n'\nt\nhesitate\nto\nemail\nme\nat\njohn\n.\n...\ncom\nor\ncall\nat\n555\n-\n123\n-\n4567\n!\n\n\nTreebankWordTokenizer\nDo\nn't\nhesitate\nto\nemail\nme\nat\njohn.doe\n@\nexample.com\n...\n!\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n\n\n\n\n3 rows × 24 columns",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#stemming",
    "href": "index.html#stemming",
    "title": "Natural Language Processing with NLTK",
    "section": "Stemming ",
    "text": "Stemming \nStemming is the process of reducing words to their word stem or root form. It’s a rule-based process that chops off the ends of words to remove affixes. Stemming is useful for text normalization, but often produces non-dictionary words.\n\nComparing Different Stemmers\n\nfrom nltk.stem import PorterStemmer, RegexpStemmer, SnowballStemmer\n\n# Create sample words to compare stemmers\nwords = [\"fearly\", \"running\", \"ran\", \"easily\", \"fairness\", \"eating\", \"eats\", \"eater\", \"eat\", \"history\", \"historical\", \"congratulations\", \"sliding\", \"comfortable\"]\n\n\nimport pandas as pd \n# Initialize stemmers\nporter_stemmer = PorterStemmer()\nregexp_stemmer = RegexpStemmer('ing$|s$|e$|able', min=4)\nsnowball_stemmer = SnowballStemmer(language=\"english\")\n\n# Create comparison table\nstemming_results = {\n    'Original': words,\n    'Porter': [porter_stemmer.stem(word) for word in words],\n    'RegExp': [regexp_stemmer.stem(word) for word in words],\n    'Snowball': [snowball_stemmer.stem(word) for word in words]\n}\n\n# Display results\nstemming_df = pd.DataFrame(stemming_results)\nstemming_df\n\n\n\n\n\n\n\n\nOriginal\nPorter\nRegExp\nSnowball\n\n\n\n\n0\nfearly\nfearli\nfearly\nfear\n\n\n1\nrunning\nrun\nrunn\nrun\n\n\n2\nran\nran\nran\nran\n\n\n3\neasily\neasili\neasily\neasili\n\n\n4\nfairness\nfair\nfairnes\nfair\n\n\n5\neating\neat\neat\neat\n\n\n6\neats\neat\neat\neat\n\n\n7\neater\neater\neater\neater\n\n\n8\neat\neat\neat\neat\n\n\n9\nhistory\nhistori\nhistory\nhistori\n\n\n10\nhistorical\nhistor\nhistorical\nhistor\n\n\n11\ncongratulations\ncongratul\ncongratulation\ncongratul\n\n\n12\nsliding\nslide\nslid\nslide\n\n\n13\ncomfortable\ncomfort\ncomfort\ncomfort\n\n\n\n\n\n\n\n\n\nStemming Analysis\nAs you can see from the results:\n\nPorter Stemmer: One of the oldest and simplest stemmers, it applies a set of rules to remove suffixes.\nRegExp Stemmer: Uses regular expressions to strip specified patterns from the end of words. It’s simple but less comprehensive.\nSnowball Stemmer: An improved version of the Porter algorithm, also known as Porter2, offering better accuracy for English and support for multiple languages.\n\nNotice how stemming can sometimes produce non-dictionary words (e.g., “histori” for “history”). This is one of the main drawbacks of stemming compared to lemmatization.",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#lemmatization",
    "href": "index.html#lemmatization",
    "title": "Natural Language Processing with NLTK",
    "section": "Lemmatization ",
    "text": "Lemmatization \nLemmatization is similar to stemming, but it reduces words to their dictionary form (lemma) rather than just chopping off affixes. It considers the morphological analysis of the words and produces actual dictionary words.\nLemmatization is often preferred for applications like chatbots, Q&A systems, and text summarization because it preserves the meaning of words.\n\nfrom nltk.stem import WordNetLemmatizer\n\n# Initialize the WordNet Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n\nimport pandas as pd\n# Lemmatization with different POS (Part-of-Speech) tags\n# POS tags: n-noun, v-verb, a-adjective, r-adverb\npos_tags = {'n': 'noun', 'v': 'verb', 'a': 'adjective', 'r': 'adverb'}\n\n# Test words for lemmatization\nlemma_words = [\"running\", \"ran\", \"better\", \"studies\", \"studied\", \"feet\", \"children\", \"geese\", \"mice\", \"are\", \"is\", \"was\", \"fairly\"]\n\n# Create comparison table for different POS tags\nlemma_results = {'Original': lemma_words}\n\nfor pos_tag, name in pos_tags.items():\n    lemma_results[f'Lemma ({name})'] = [lemmatizer.lemmatize(word, pos=pos_tag) for word in lemma_words]\n\n# Display results\npd.DataFrame(lemma_results)\n\n\n\n\n\n\n\n\nOriginal\nLemma (noun)\nLemma (verb)\nLemma (adjective)\nLemma (adverb)\n\n\n\n\n0\nrunning\nrunning\nrun\nrunning\nrunning\n\n\n1\nran\nran\nrun\nran\nran\n\n\n2\nbetter\nbetter\nbetter\ngood\nwell\n\n\n3\nstudies\nstudy\nstudy\nstudies\nstudies\n\n\n4\nstudied\nstudied\nstudy\nstudied\nstudied\n\n\n5\nfeet\nfoot\nfeet\nfeet\nfeet\n\n\n6\nchildren\nchild\nchildren\nchildren\nchildren\n\n\n7\ngeese\ngoose\ngeese\ngeese\ngeese\n\n\n8\nmice\nmouse\nmice\nmice\nmice\n\n\n9\nare\nare\nbe\nare\nare\n\n\n10\nis\nis\nbe\nis\nis\n\n\n11\nwas\nwa\nbe\nwas\nwas\n\n\n12\nfairly\nfairly\nfairly\nfairly\nfairly\n\n\n\n\n\n\n\n\nStemming vs. Lemmatization\nLet’s compare stemming and lemmatization side by side to see the differences:\n\n# Compare stemming vs lemmatization\ncompare_words = [\"running\", \"better\", \"studies\", \"feet\", \"wolves\", \"are\", \"historically\"]\n\ncomparison = {\n    'Original': compare_words,\n    'Porter Stemmer': [porter_stemmer.stem(word) for word in compare_words],\n    'Snowball Stemmer': [snowball_stemmer.stem(word) for word in compare_words],\n    'Lemmatization (verb)': [lemmatizer.lemmatize(word, pos='v') for word in compare_words],\n    'Lemmatization (noun)': [lemmatizer.lemmatize(word, pos='n') for word in compare_words]\n}\n\npd.DataFrame(comparison)\n\n\n\n\n\n\n\n\nOriginal\nPorter Stemmer\nSnowball Stemmer\nLemmatization (verb)\nLemmatization (noun)\n\n\n\n\n0\nrunning\nrun\nrun\nrun\nrunning\n\n\n1\nbetter\nbetter\nbetter\nbetter\nbetter\n\n\n2\nstudies\nstudi\nstudi\nstudy\nstudy\n\n\n3\nfeet\nfeet\nfeet\nfeet\nfoot\n\n\n4\nwolves\nwolv\nwolv\nwolves\nwolf\n\n\n5\nare\nare\nare\nbe\nare\n\n\n6\nhistorically\nhistor\nhistor\nhistorically\nhistorically",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#stopword-removal",
    "href": "index.html#stopword-removal",
    "title": "Natural Language Processing with NLTK",
    "section": "Stopword Removal ",
    "text": "Stopword Removal \nStopwords are common words like “the”, “a”, “an”, “in” that usually don’t carry much meaning in text analysis. Removing them can help reduce noise in text processing.\n\n# Sample paragraph for stopword removal\nparagraph = \"\"\"On July 16, 1969, the Apollo 11 spacecraft launched from the Kennedy Space Center in Florida. Its mission was to go where no human being had gone before—the moon! The crew consisted of Neil Armstrong, Michael Collins, and Buzz Aldrin. The spacecraft landed on the moon in the Sea of Tranquility, a basaltic flood plain, on July 20, 1969. The moonwalk took place the following day. On July 21, 1969, at precisely 10:56 EDT, Commander Neil Armstrong emerged from the Lunar Module and took his famous first step onto the moon's surface. He declared, \n It was a monumental moment in human history!\"\"\"\n\n\nfrom nltk.corpus import stopwords\n\n# Get English stopwords\nstop_words = stopwords.words('english')\n\n# Display first 20 stopwords\nprint(f\"Total English stopwords: {len(stop_words)}\")\nprint(f\"Sample stopwords: {stop_words[:20]}\")\n\nTotal English stopwords: 198\nSample stopwords: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n\n\n\n# Process text with and without stopwords removal\n# Tokenize the paragraph into sentences\nsentences = nltk.sent_tokenize(paragraph)\n\n# Initialize lists for processed sentences\nwith_stopwords = []\nwithout_stopwords = []\n\n# Process each sentence\nfor sentence in sentences[:3]:  # Process first 3 sentences for brevity\n    words = nltk.word_tokenize(sentence)\n    \n    # Keep all words\n    with_stopwords.append(' '.join(words))\n    \n    # Remove stopwords\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    without_stopwords.append(' '.join(filtered_words))\n\n# Create DataFrame for comparison\nstopword_df = pd.DataFrame({\n    'Original with Stopwords': with_stopwords,\n    'After Stopwords Removal': without_stopwords\n})\n\nstopword_df\n\n\n\n\n\n\n\n\nOriginal with Stopwords\nAfter Stopwords Removal\n\n\n\n\n0\nOn July 16 , 1969 , the Apollo 11 spacecraft l...\nJuly 16 , 1969 , Apollo 11 spacecraft launched...\n\n\n1\nIts mission was to go where no human being had...\nmission go human gone before—the moon !\n\n\n2\nThe crew consisted of Neil Armstrong , Michael...\ncrew consisted Neil Armstrong , Michael Collin...\n\n\n\n\n\n\n\n\nComplete Text Processing Pipeline\nLet’s put everything together to create a complete text processing pipeline that includes tokenization, stopword removal, and either stemming or lemmatization.\n\ndef process_text(text, use_stemming=True, use_lemmatization=False):\n    \"\"\"Process text using a complete NLP pipeline\n    \n    Args:\n        text (str): Input text to process\n        use_stemming (bool): Whether to apply stemming\n        use_lemmatization (bool): Whether to apply lemmatization\n        \n    Returns:\n        list: List of processed sentences\n    \"\"\"\n    # Tokenize into sentences\n    sentences = nltk.sent_tokenize(text)\n    processed_sentences = []\n    \n    for sentence in sentences:\n        # Tokenize into words\n        words = nltk.word_tokenize(sentence)\n        \n        # Remove stopwords\n        filtered_words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word.isalnum()]\n        \n        # Apply stemming or lemmatization\n        if use_stemming:\n            processed_words = [snowball_stemmer.stem(word) for word in filtered_words]\n        elif use_lemmatization:\n            processed_words = [lemmatizer.lemmatize(word, pos='v') for word in filtered_words]\n        else:\n            processed_words = filtered_words\n            \n        processed_sentences.append(' '.join(processed_words))\n        \n    return processed_sentences\n\n# Process the paragraph\nstemmed_text = process_text(paragraph, use_stemming=True, use_lemmatization=False)\nlemmatized_text = process_text(paragraph, use_stemming=False, use_lemmatization=True)\n\n# Display first 3 processed sentences\nfor i, (stem, lemma) in enumerate(zip(stemmed_text[:3], lemmatized_text[:3])):\n    print(f\"Sentence {i+1}:\")\n    print(f\"  Stemmed: {stem}\")\n    print(f\"  Lemmatized: {lemma}\")\n    print()\n\nSentence 1:\n  Stemmed: juli 16 1969 apollo 11 spacecraft launch kennedi space center florida\n  Lemmatized: july 16 1969 apollo 11 spacecraft launch kennedy space center florida\n\nSentence 2:\n  Stemmed: mission go human gone moon\n  Lemmatized: mission go human go moon\n\nSentence 3:\n  Stemmed: crew consist neil armstrong michael collin buzz aldrin\n  Lemmatized: crew consist neil armstrong michael collins buzz aldrin",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#part-of-speech-pos-tagging",
    "href": "index.html#part-of-speech-pos-tagging",
    "title": "Natural Language Processing with NLTK",
    "section": "Part-of-Speech (POS) Tagging ",
    "text": "Part-of-Speech (POS) Tagging \nPOS tagging is the process of marking words in a text with their corresponding part of speech (noun, verb, adjective, etc.). It’s an essential step for many NLP applications.\n\n# Common POS tags in NLTK\npos_tags_info = {\n    'CC': 'Coordinating conjunction',\n    'CD': 'Cardinal digit',\n    'DT': 'Determiner',\n    'EX': 'Existential there (\"there is\")',\n    'FW': 'Foreign word',\n    'IN': 'Preposition/subordinating conjunction',\n    'JJ': 'Adjective',\n    'JJR': 'Adjective, comparative (\"bigger\")',\n    'JJS': 'Adjective, superlative (\"biggest\")',\n    'LS': 'List marker',\n    'MD': 'Modal (could, will)',\n    'NN': 'Noun, singular',\n    'NNS': 'Noun plural',\n    'NNP': 'Proper noun, singular',\n    'NNPS': 'Proper noun, plural',\n    'PDT': 'Predeterminer',\n    'POS': 'Possessive ending',\n    'PRP': 'Personal pronoun (I, he, she)',\n    'PRP$': 'Possessive pronoun (my, his, hers)',\n    'RB': 'Adverb',\n    'RBR': 'Adverb, comparative',\n    'RBS': 'Adverb, superlative',\n    'RP': 'Particle',\n    'TO': 'to',\n    'UH': 'Interjection',\n    'VB': 'Verb, base form',\n    'VBD': 'Verb, past tense',\n    'VBG': 'Verb, gerund/present participle',\n    'VBN': 'Verb, past participle',\n    'VBP': 'Verb, sing. present, non-3d',\n    'VBZ': 'Verb, 3rd person sing. present',\n    'WDT': 'Wh-determiner (which)',\n    'WP': 'Wh-pronoun (who, what)',\n    'WP$': 'Possessive wh-pronoun (whose)',\n    'WRB': 'Wh-adverb (where, when)'\n}\n\n# Display POS tag information as a table\npos_df = pd.DataFrame([(tag, desc) for tag, desc in pos_tags_info.items()], \n                      columns=['Tag', 'Description'])\npos_df\n\n\n\n\n\n\n\n\nTag\nDescription\n\n\n\n\n0\nCC\nCoordinating conjunction\n\n\n1\nCD\nCardinal digit\n\n\n2\nDT\nDeterminer\n\n\n3\nEX\nExistential there (\"there is\")\n\n\n4\nFW\nForeign word\n\n\n5\nIN\nPreposition/subordinating conjunction\n\n\n6\nJJ\nAdjective\n\n\n7\nJJR\nAdjective, comparative (\"bigger\")\n\n\n8\nJJS\nAdjective, superlative (\"biggest\")\n\n\n9\nLS\nList marker\n\n\n10\nMD\nModal (could, will)\n\n\n11\nNN\nNoun, singular\n\n\n12\nNNS\nNoun plural\n\n\n13\nNNP\nProper noun, singular\n\n\n14\nNNPS\nProper noun, plural\n\n\n15\nPDT\nPredeterminer\n\n\n16\nPOS\nPossessive ending\n\n\n17\nPRP\nPersonal pronoun (I, he, she)\n\n\n18\nPRP$\nPossessive pronoun (my, his, hers)\n\n\n19\nRB\nAdverb\n\n\n20\nRBR\nAdverb, comparative\n\n\n21\nRBS\nAdverb, superlative\n\n\n22\nRP\nParticle\n\n\n23\nTO\nto\n\n\n24\nUH\nInterjection\n\n\n25\nVB\nVerb, base form\n\n\n26\nVBD\nVerb, past tense\n\n\n27\nVBG\nVerb, gerund/present participle\n\n\n28\nVBN\nVerb, past participle\n\n\n29\nVBP\nVerb, sing. present, non-3d\n\n\n30\nVBZ\nVerb, 3rd person sing. present\n\n\n31\nWDT\nWh-determiner (which)\n\n\n32\nWP\nWh-pronoun (who, what)\n\n\n33\nWP$\nPossessive wh-pronoun (whose)\n\n\n34\nWRB\nWh-adverb (where, when)\n\n\n\n\n\n\n\n\n# Example sentences for POS tagging\nexample_sentences = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"I am studying natural language processing.\",\n    \"She walked to the store, but it was closed.\"\n]\n\n# Perform POS tagging\nfor i, sentence in enumerate(example_sentences, 1):\n    # Tokenize and tag words\n    words = nltk.word_tokenize(sentence)\n    tagged = nltk.pos_tag(words)\n    \n    # Create a visualization of the tagged sentence\n    print(f\"Sentence {i}: {sentence}\")\n    \n    # Display tagged words in a table format\n    tagged_df = pd.DataFrame(tagged, columns=['Word', 'POS Tag'])\n    tagged_df['Description'] = tagged_df['POS Tag'].map(lambda tag: pos_tags_info.get(tag, 'Unknown'))\n    display(tagged_df)\n    print(\"\\n\")\n\nSentence 1: The quick brown fox jumps over the lazy dog.\n\n\n\n\n\n\n\n\n\nWord\nPOS Tag\nDescription\n\n\n\n\n0\nThe\nDT\nDeterminer\n\n\n1\nquick\nJJ\nAdjective\n\n\n2\nbrown\nNN\nNoun, singular\n\n\n3\nfox\nNN\nNoun, singular\n\n\n4\njumps\nVBZ\nVerb, 3rd person sing. present\n\n\n5\nover\nIN\nPreposition/subordinating conjunction\n\n\n6\nthe\nDT\nDeterminer\n\n\n7\nlazy\nJJ\nAdjective\n\n\n8\ndog\nNN\nNoun, singular\n\n\n9\n.\n.\nUnknown\n\n\n\n\n\n\n\n\n\nSentence 2: I am studying natural language processing.\n\n\n\n\n\n\n\n\n\nWord\nPOS Tag\nDescription\n\n\n\n\n0\nI\nPRP\nPersonal pronoun (I, he, she)\n\n\n1\nam\nVBP\nVerb, sing. present, non-3d\n\n\n2\nstudying\nVBG\nVerb, gerund/present participle\n\n\n3\nnatural\nJJ\nAdjective\n\n\n4\nlanguage\nNN\nNoun, singular\n\n\n5\nprocessing\nNN\nNoun, singular\n\n\n6\n.\n.\nUnknown\n\n\n\n\n\n\n\n\n\nSentence 3: She walked to the store, but it was closed.\n\n\n\n\n\n\n\n\n\nWord\nPOS Tag\nDescription\n\n\n\n\n0\nShe\nPRP\nPersonal pronoun (I, he, she)\n\n\n1\nwalked\nVBD\nVerb, past tense\n\n\n2\nto\nTO\nto\n\n\n3\nthe\nDT\nDeterminer\n\n\n4\nstore\nNN\nNoun, singular\n\n\n5\n,\n,\nUnknown\n\n\n6\nbut\nCC\nCoordinating conjunction\n\n\n7\nit\nPRP\nPersonal pronoun (I, he, she)\n\n\n8\nwas\nVBD\nVerb, past tense\n\n\n9\nclosed\nVBN\nVerb, past participle\n\n\n10\n.\n.\nUnknown",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#named-entity-recognition-ner",
    "href": "index.html#named-entity-recognition-ner",
    "title": "Natural Language Processing with NLTK",
    "section": "Named Entity Recognition (NER) ",
    "text": "Named Entity Recognition (NER) \nNamed Entity Recognition is the process of identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, time expressions, quantities, etc.\n\n# Example sentences for NER\nner_examples = [\n    \"The Eiffel Tower stands on four lattice-girder piers that taper inward and join to form a single large vertical tower.\",\n    \"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.\",\n    \"Barack Obama was born in Hawaii and served as the 44th president of the United States from 2009 to 2017.\"\n]\n\n\n# Process examples with NER\nfor i, example in enumerate(ner_examples, 1):\n    # Tokenize and tag\n    words = nltk.word_tokenize(example)\n    pos_tags = nltk.pos_tag(words)\n    \n    # Apply NER\n    ner_tree = nltk.ne_chunk(pos_tags)\n    \n    print(f\"Example {i}: {example}\")\n    print(\"\\nNamed Entities:\")\n    \n    # Extract and print named entities\n    named_entities = []\n    for chunk in ner_tree:\n        if hasattr(chunk, 'label'):\n            entity_name = ' '.join(c[0] for c in chunk)\n            entity_type = chunk.label()\n            named_entities.append((entity_name, entity_type))\n    \n    if named_entities:\n        entities_df = pd.DataFrame(named_entities, columns=['Entity', 'Type'])\n        display(entities_df)\n    else:\n        print(\"No named entities found\")\n    \n    print(\"\\n\")\n\nExample 1: The Eiffel Tower stands on four lattice-girder piers that taper inward and join to form a single large vertical tower.\n\nNamed Entities:\n\n\n\n\n\n\n\n\n\nEntity\nType\n\n\n\n\n0\nEiffel Tower\nORGANIZATION\n\n\n\n\n\n\n\n\n\nExample 2: Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.\n\nNamed Entities:\nExample 2: Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.\n\nNamed Entities:\n\n\n\n\n\n\n\n\n\nEntity\nType\n\n\n\n\n0\nApple\nPERSON\n\n\n1\nInc.\nORGANIZATION\n\n\n2\nSteve Jobs\nPERSON\n\n\n3\nSteve Wozniak\nPERSON\n\n\n4\nRonald Wayne\nPERSON\n\n\n\n\n\n\n\n\n\nExample 3: Barack Obama was born in Hawaii and served as the 44th president of the United States from 2009 to 2017.\n\nNamed Entities:\nExample 3: Barack Obama was born in Hawaii and served as the 44th president of the United States from 2009 to 2017.\n\nNamed Entities:\n\n\n\n\n\n\n\n\n\nEntity\nType\n\n\n\n\n0\nBarack\nPERSON\n\n\n1\nObama\nPERSON\n\n\n2\nHawaii\nGPE\n\n\n3\nUnited States\nGPE\n\n\n\n\n\n\n\n\n\n\n\n\n# Visualize NER Tree (if svgling is installed)\ntry:\n    import svgling\n    \n    # Use the second example for visualization\n    example = ner_examples[1]\n    words = nltk.word_tokenize(example)\n    pos_tags = nltk.pos_tag(words)\n    ner_tree = nltk.ne_chunk(pos_tags)\n    \n    print(f\"Named Entity Tree for: {example}\")\n    svgling.draw_tree(ner_tree)\nexcept ImportError:\n    print(\"To visualize NER trees, install the 'svgling' package using: pip install svgling\")\n    # Alternative visualization\n    print(ner_tree)\n\nNamed Entity Tree for: Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976.",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "index.html#text-analysis-dashboard",
    "href": "index.html#text-analysis-dashboard",
    "title": "Natural Language Processing with NLTK",
    "section": "Text Analysis Dashboard ",
    "text": "Text Analysis Dashboard \nLet’s create a comprehensive analysis of text using various NLP techniques we’ve learned.\n\ndef analyze_text(text):\n    \"\"\"Comprehensive text analysis using NLTK\"\"\"\n    from collections import Counter\n    import re\n    \n    # Basic statistics\n    sentences = nltk.sent_tokenize(text)\n    words = nltk.word_tokenize(text)\n    words_lower = [word.lower() for word in words if word.isalnum()]\n    stop_words = set(stopwords.words('english'))\n    words_no_stop = [word for word in words_lower if word not in stop_words]\n    \n    # Word frequency\n    word_freq = Counter(words_no_stop)\n    common_words = word_freq.most_common(10)\n    \n    # POS distribution\n    pos_tags = nltk.pos_tag(words_lower)\n    pos_counts = Counter([tag for _, tag in pos_tags])\n    \n    # Named entities\n    ner_tree = nltk.ne_chunk(nltk.pos_tag(words))\n    named_entities = []\n    for chunk in ner_tree:\n        if hasattr(chunk, 'label'):\n            entity_name = ' '.join(c[0] for c in chunk)\n            entity_type = chunk.label()\n            named_entities.append((entity_name, entity_type))\n    \n    # Print results\n    print(\"=== TEXT ANALYSIS DASHBOARD ===\")\n    print(f\"Text length: {len(text)} characters\")\n    print(f\"Sentences: {len(sentences)}\")\n    print(f\"Words: {len(words_lower)}\")\n    print(f\"Unique words: {len(set(words_lower))}\")\n    print(f\"Words without stopwords: {len(words_no_stop)}\")\n    \n    print(\"\\n=== MOST COMMON WORDS ===\")\n    for word, count in common_words:\n        print(f\"{word}: {count}\")\n    \n    print(\"\\n=== PART OF SPEECH DISTRIBUTION ===\")\n    for pos, count in pos_counts.most_common(5):\n        print(f\"{pos} ({pos_tags_info.get(pos, 'Unknown')}): {count}\")\n    \n    print(\"\\n=== NAMED ENTITIES ===\")\n    if named_entities:\n        entities_df = pd.DataFrame(named_entities, columns=['Entity', 'Type'])\n        display(entities_df)\n    else:\n        print(\"No named entities found\")\n    \n    # Generate word cloud if matplotlib is available\n    try:\n        from wordcloud import WordCloud\n        \n        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(words_no_stop))\n        \n        plt.figure(figsize=(10, 5))\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis('off')\n        plt.title('Word Cloud')\n        plt.show()\n    except ImportError:\n        print(\"\\nInstall wordcloud package for word cloud visualization: pip install wordcloud\")\n\n# Run analysis on the Apollo 11 paragraph\nanalyze_text(paragraph)\n\n=== TEXT ANALYSIS DASHBOARD ===\nText length: 593 characters\nSentences: 7\nWords: 100\nUnique words: 70\nWords without stopwords: 63\n\n=== MOST COMMON WORDS ===\njuly: 3\n1969: 3\nmoon: 3\nspacecraft: 2\nhuman: 2\nneil: 2\narmstrong: 2\ntook: 2\n16: 1\napollo: 1\n\n=== PART OF SPEECH DISTRIBUTION ===\nNN (Noun, singular): 32\nIN (Preposition/subordinating conjunction): 14\nDT (Determiner): 13\nVBD (Verb, past tense): 9\nJJ (Adjective): 8\n\n=== NAMED ENTITIES ===\n\n\n\n\n\n\n\n\n\nEntity\nType\n\n\n\n\n0\nKennedy Space Center\nFACILITY\n\n\n1\nFlorida\nGPE\n\n\n2\nNeil Armstrong\nPERSON\n\n\n3\nMichael Collins\nPERSON\n\n\n4\nBuzz Aldrin\nPERSON\n\n\n5\nSea\nORGANIZATION\n\n\n6\nTranquility\nGPE\n\n\n7\nCommander Neil Armstrong\nORGANIZATION\n\n\n8\nLunar Module\nORGANIZATION\n\n\n\n\n\n\n\n\nInstall wordcloud package for word cloud visualization: pip install wordcloud",
    "crumbs": [
      "NLTK-NOTES",
      "Natural Language Processing with NLTK"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Fevzi KILAS",
    "section": "",
    "text": "Data Scientist and AI Researcher\n\n\nPersonal Website\n\n\nFevzi KILAS Website\n\n\nGitHub\n\n\nHugging Face\n\n\nLinkedIn: Fevzi KILAS",
    "crumbs": [
      "NLTK-NOTES",
      "Fevzi KILAS"
    ]
  }
]